{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098eeb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee89467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Teste_Matching.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e7197b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tratamento</th>\n",
       "      <th>Convidado</th>\n",
       "      <th>Outcoming_Y-1</th>\n",
       "      <th>Outcoming_Y</th>\n",
       "      <th>Timing1_Y</th>\n",
       "      <th>Timing2_Y</th>\n",
       "      <th>Position_Y-1</th>\n",
       "      <th>Position_Y</th>\n",
       "      <th>State_Y-1</th>\n",
       "      <th>State_Y</th>\n",
       "      <th>Change_dept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>95</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Tratamento  Convidado  Outcoming_Y-1  Outcoming_Y  Timing1_Y  \\\n",
       "0   1           0          0             51           53          9   \n",
       "1   2           1          1             79           95          3   \n",
       "2   3           0          0             80           71          2   \n",
       "3   4           1          1             65           71          9   \n",
       "4   5           0          0              2            6          4   \n",
       "\n",
       "   Timing2_Y  Position_Y-1  Position_Y  State_Y-1  State_Y  Change_dept  \n",
       "0          7             1           1          7        5            0  \n",
       "1          4             2           2          7        7            0  \n",
       "2          9             2           2          5        5            0  \n",
       "3          7             1           2          2        2            0  \n",
       "4          3             2           3          3        3            1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "103c227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Scanner de associações (T <-> X, Y <-> X) para decisão de inclusão\n",
    "# ===============================================================\n",
    "# O que faz:\n",
    "# - Para cada feature X:\n",
    "#   * Se X é numérica:\n",
    "#       - com Tratamento (binário): correlação ponto-biserial (Pearson com binário) + p-valor; SMD opcional\n",
    "#       - com Outcome:\n",
    "#           - se Y binário: ponto-biserial\n",
    "#           - se Y contínuo: Pearson + Spearman\n",
    "#   * Se X é categórica:\n",
    "#       - com Tratamento/Y binário: qui-quadrado + Cramer's V (com correção de viés)\n",
    "#       - com Y contínuo: ANOVA one-way + eta^2\n",
    "#   * Mutual Information (não-linear): MI(X;T) e MI(X;Y)\n",
    "# - Classifica:\n",
    "#     confounder_candidate: associado a T e Y\n",
    "#     instrument_like     : associado a T e não a Y (evitar, a menos que queira instrumentar)\n",
    "#     prognostic_only     : associado a Y e não a T (bom p/ precisão)\n",
    "#     weak/irrelevant     : sem associações fortes\n",
    "# - Parâmetros de decisão (thresholds) ajustáveis.\n",
    "#\n",
    "# Pré-requisitos: pandas, numpy, scipy, scikit-learn, statsmodels (já usa no notebook)\n",
    "# ===============================================================\n",
    "\n",
    "# --------- helpers de medidas ---------\n",
    "def cramers_v_corrected(confusion):\n",
    "    \"\"\"Cramer's V com correção de viés (Bergsma, 2013).\"\"\"\n",
    "    chi2 = stats.chi2_contingency(confusion, correction=False)[0]\n",
    "    n = confusion.sum().sum()\n",
    "    r, k = confusion.shape\n",
    "    phi2 = chi2 / n\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    denom = min((kcorr-1), (rcorr-1))\n",
    "    return np.sqrt(phi2corr / denom) if denom > 0 else 0.0\n",
    "\n",
    "def eta_squared_anova(groups):\n",
    "    \"\"\"Eta^2 da ANOVA: SSeffect / SStotal.\"\"\"\n",
    "    # groups: lista de arrays por nível categórico\n",
    "    all_vals = np.concatenate(groups)\n",
    "    grand_mean = np.mean(all_vals)\n",
    "    ss_total = np.sum((all_vals - grand_mean)**2)\n",
    "    ss_between = np.sum([len(g)*(np.mean(g)-grand_mean)**2 for g in groups])\n",
    "    return ss_between / ss_total if ss_total > 0 else 0.0\n",
    "\n",
    "def is_binary_series(s):\n",
    "    vals = pd.unique(s.dropna())\n",
    "    return len(vals) == 2\n",
    "\n",
    "def _mutual_info_safe(x, y, y_is_binary, discrete_x):\n",
    "    \"\"\"MI robusto a NaN, escolhe função apropriada.\"\"\"\n",
    "    df_xy = pd.DataFrame({'x': x, 'y': y}).dropna()\n",
    "    if df_xy.empty:\n",
    "        return np.nan\n",
    "    X = df_xy[['x']].values\n",
    "    yy = df_xy['y'].values\n",
    "    if y_is_binary or (is_binary_series(pd.Series(yy))):\n",
    "        # classif\n",
    "        if discrete_x:\n",
    "            # scikit assume inteiros para discretos; garantir int\n",
    "            X = X.astype(int)\n",
    "        return float(mutual_info_classif(X, yy, discrete_features=discrete_x, random_state=42)[0])\n",
    "    else:\n",
    "        # regressão\n",
    "        if discrete_x:\n",
    "            X = X.astype(int)\n",
    "        return float(mutual_info_regression(X, yy, discrete_features=discrete_x, random_state=42)[0])\n",
    "\n",
    "def _label_encode_safe(s):\n",
    "    \"\"\"LabelEncode em categórica preservando NaN (removidos nas análises).\"\"\"\n",
    "    le = LabelEncoder()\n",
    "    # filtra NaN e ajusta\n",
    "    non_na = s.dropna().astype(str)\n",
    "    le.fit(non_na)\n",
    "    out = pd.Series(index=s.index, dtype=float)\n",
    "    out.loc[non_na.index] = le.transform(non_na).astype(float)\n",
    "    return out\n",
    "\n",
    "# --------- função principal ---------\n",
    "def scan_associations(\n",
    "    df,\n",
    "    treatment_col='Tratamento',\n",
    "    outcome_col='Outcoming_Y',\n",
    "    candidate_features=None,\n",
    "    alpha=0.05,\n",
    "    thr_corr_T=0.10,       # |r| ou V >= 0.10 ~ efeito pequeno\n",
    "    thr_corr_Y=0.10,       # |r|/V para Y binário; para ANOVA usar thr_eta2\n",
    "    thr_eta2=0.02,         # eta^2 ~ 0.01 pequeno, 0.06 médio; aqui 0.02 conservador\n",
    "    compute_smd=True,\n",
    "    drop_const_dummies=True # remove dummies constantes (0/1) sem variação\n",
    "):\n",
    "    \"\"\"\n",
    "    Retorna DataFrame com:\n",
    "      feature, type, n_T, assoc_T_metric, p_T, n_Y, assoc_Y_metric, p_Y,\n",
    "      MI_T, MI_Y, classification\n",
    "    \"\"\"\n",
    "    T = df[treatment_col]\n",
    "    Y = df[outcome_col]\n",
    "    y_is_binary = is_binary_series(Y)\n",
    "\n",
    "    # Se candidato não veio, use todas as colunas exceto T e Y\n",
    "    if candidate_features is None:\n",
    "        candidate_features = [c for c in df.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for col in candidate_features:\n",
    "        if col in [treatment_col, outcome_col]:\n",
    "            continue\n",
    "\n",
    "        s = df[col]\n",
    "        # descartar dummies constantes (evita erros e p-valores bizarros)\n",
    "        if drop_const_dummies and s.nunique(dropna=True) <= 1:\n",
    "            continue\n",
    "\n",
    "        is_numeric = pd.api.types.is_numeric_dtype(s)\n",
    "        # Para categóricas, vamos criar versão codificada para MI e ANOVA\n",
    "        s_enc = s if is_numeric else _label_encode_safe(s)\n",
    "\n",
    "        # ---------- Associação com Tratamento (T binário) ----------\n",
    "        # Métrica + p-valor\n",
    "        if is_numeric:\n",
    "            # ponto-biserial (Pearson com T binário)\n",
    "            df_tx = pd.DataFrame({'x': s, 't': T}).dropna()\n",
    "            n_T = len(df_tx)\n",
    "            if n_T > 2 and df_tx['x'].var() > 0 and df_tx['t'].var() > 0:\n",
    "                r_T, p_T = stats.pointbiserialr(df_tx['t'], df_tx['x'])\n",
    "                assoc_T = abs(r_T)\n",
    "            else:\n",
    "                r_T, p_T, assoc_T, n_T = np.nan, np.nan, np.nan, n_T\n",
    "            # SMD opcional (tratado vs controle)\n",
    "            if compute_smd and n_T > 2 and df_tx['t'].nunique() == 2:\n",
    "                x1 = df_tx.loc[df_tx['t']==1, 'x']; x0 = df_tx.loc[df_tx['t']==0, 'x']\n",
    "                v1, v0 = x1.var(ddof=1), x0.var(ddof=1)\n",
    "                smd = (x1.mean() - x0.mean()) / np.sqrt(0.5*(v1+v0) + 1e-12) if v1>0 and v0>0 else np.nan\n",
    "            else:\n",
    "                smd = np.nan\n",
    "        else:\n",
    "            # qui-quadrado + Cramer's V\n",
    "            ct = pd.crosstab(T, s, dropna=True)\n",
    "            n_T = ct.values.sum()\n",
    "            try:\n",
    "                chi2, p_T, _, _ = stats.chi2_contingency(ct, correction=False)\n",
    "                assoc_T = cramers_v_corrected(ct)\n",
    "            except Exception:\n",
    "                chi2, p_T, assoc_T = np.nan, np.nan, np.nan\n",
    "            smd = np.nan\n",
    "\n",
    "        # Mutual information com T\n",
    "        try:\n",
    "            MI_T = _mutual_info_safe(s_enc, T, y_is_binary=True, discrete_x=not is_numeric)\n",
    "        except Exception:\n",
    "            MI_T = np.nan\n",
    "\n",
    "        # ---------- Associação com Y (contínuo ou binário) ----------\n",
    "        if y_is_binary:\n",
    "            # Y binário\n",
    "            if is_numeric:\n",
    "                df_yx = pd.DataFrame({'x': s, 'y': Y}).dropna()\n",
    "                n_Y = len(df_yx)\n",
    "                if n_Y > 2 and df_yx['x'].var() > 0 and df_yx['y'].var() > 0:\n",
    "                    r_Y, p_Y = stats.pointbiserialr(df_yx['y'], df_yx['x'])\n",
    "                    assoc_Y = abs(r_Y)\n",
    "                else:\n",
    "                    r_Y, p_Y, assoc_Y, n_Y = np.nan, np.nan, np.nan, n_Y\n",
    "            else:\n",
    "                ct = pd.crosstab(Y, s, dropna=True)\n",
    "                n_Y = ct.values.sum()\n",
    "                try:\n",
    "                    chi2, p_Y, _, _ = stats.chi2_contingency(ct, correction=False)\n",
    "                    assoc_Y = cramers_v_corrected(ct)\n",
    "                except Exception:\n",
    "                    p_Y, assoc_Y = np.nan, np.nan\n",
    "        else:\n",
    "            # Y contínuo\n",
    "            if is_numeric:\n",
    "                df_yx = pd.DataFrame({'x': s, 'y': Y}).dropna()\n",
    "                n_Y = len(df_yx)\n",
    "                if n_Y > 2 and df_yx['x'].var() > 0 and df_yx['y'].var() > 0:\n",
    "                    pear_r, pear_p = stats.pearsonr(df_yx['x'], df_yx['y'])\n",
    "                    spear_r, spear_p = stats.spearmanr(df_yx['x'], df_yx['y'])\n",
    "                    # usaremos |Pearson| como métrica principal; Spearman como apoio\n",
    "                    assoc_Y, p_Y = abs(pear_r), pear_p\n",
    "                else:\n",
    "                    assoc_Y, p_Y, n_Y = np.nan, np.nan, n_Y\n",
    "            else:\n",
    "                # ANOVA: Y ~ X_cat\n",
    "                df_yx = pd.DataFrame({'x': s, 'y': Y}).dropna()\n",
    "                n_Y = len(df_yx)\n",
    "                groups = [g.values for _, g in df_yx.groupby('x')['y']]\n",
    "                if len(groups) >= 2 and all(len(g) > 1 for g in groups):\n",
    "                    try:\n",
    "                        F, p_Y = stats.f_oneway(*groups)\n",
    "                    except Exception:\n",
    "                        # f_oneway pode falhar com variâncias muito desiguais; caia fora educadamente\n",
    "                        F, p_Y = np.nan, np.nan\n",
    "                    eta2 = eta_squared_anova(groups) if all(len(g)>1 for g in groups) else np.nan\n",
    "                    assoc_Y = eta2  # para categórica vs contínuo, use eta^2 como força\n",
    "                else:\n",
    "                    assoc_Y, p_Y = np.nan, np.nan\n",
    "\n",
    "        # Mutual information com Y\n",
    "        try:\n",
    "            MI_Y = _mutual_info_safe(s_enc, Y, y_is_binary=y_is_binary, discrete_x=not is_numeric)\n",
    "        except Exception:\n",
    "            MI_Y = np.nan\n",
    "\n",
    "        # ---------- Regras de decisão ----------\n",
    "        # Sinaliza associação \"forte\" com T e Y conforme tipo de teste\n",
    "        assoc_T_strong = (assoc_T >= thr_corr_T) and (p_T < alpha) if not np.isnan(assoc_T) and not np.isnan(p_T) else False\n",
    "\n",
    "        if y_is_binary:\n",
    "            assoc_Y_strong = (assoc_Y >= thr_corr_Y) and (p_Y < alpha) if not np.isnan(assoc_Y) and not np.isnan(p_Y) else False\n",
    "        else:\n",
    "            # se categórica vs contínuo usamos eta^2; para numérica vs contínuo usamos |r|\n",
    "            if not is_numeric:\n",
    "                assoc_Y_strong = (assoc_Y >= thr_eta2) and (p_Y < alpha) if not np.isnan(assoc_Y) and not np.isnan(p_Y) else False\n",
    "            else:\n",
    "                assoc_Y_strong = (assoc_Y >= thr_corr_Y) and (p_Y < alpha) if not np.isnan(assoc_Y) and not np.isnan(p_Y) else False\n",
    "\n",
    "        if assoc_T_strong and assoc_Y_strong:\n",
    "            label = 'confounder_candidate (INCLUIR)'\n",
    "        elif assoc_T_strong and not assoc_Y_strong:\n",
    "            label = 'instrument_like (EVITAR na modelagem causal; pode servir como IV)'\n",
    "        elif (not assoc_T_strong) and assoc_Y_strong:\n",
    "            label = 'prognostic_only (BOM p/ precisão)'\n",
    "        else:\n",
    "            label = 'weak/irrelevant (OPCIONAL)'\n",
    "\n",
    "        results.append({\n",
    "            'feature'        : col,\n",
    "            'type'           : ('numeric' if is_numeric else 'categorical'),\n",
    "            'n_T'            : int(n_T),\n",
    "            'assoc_T_metric' : assoc_T,    # |r| (num) ou Cramer's V (cat)\n",
    "            'p_T'            : p_T,\n",
    "            'SMD_pre'        : smd if compute_smd else np.nan,\n",
    "            'n_Y'            : int(n_Y) if 'n_Y' in locals() else np.nan,\n",
    "            'assoc_Y_metric' : assoc_Y,    # |r| (num) ou eta^2 (cat vs cont) / V (cat vs bin)\n",
    "            'p_Y'            : p_Y,\n",
    "            'MI_T'           : MI_T,\n",
    "            'MI_Y'           : MI_Y,\n",
    "            'classification' : label\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(results)\n",
    "\n",
    "    # Ordena: 1) confounders, 2) prognósticas, 3) instrument-like, 4) fracas\n",
    "    cat_order = pd.CategoricalDtype(\n",
    "        ['confounder_candidate (INCLUIR)',\n",
    "         'prognostic_only (BOM p/ precisão)',\n",
    "         'instrument_like (EVITAR na modelagem causal; pode servir como IV)',\n",
    "         'weak/irrelevant (OPCIONAL)'],\n",
    "        ordered=True\n",
    "    )\n",
    "    out['classification'] = out['classification'].astype(cat_order)\n",
    "    out = out.sort_values(['classification', 'assoc_Y_metric', 'assoc_T_metric'], ascending=[True, False, False]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c57cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         feature     type   n_T  assoc_T_metric           p_T   SMD_pre   n_Y  \\\n",
      "0  Outcoming_Y-1  numeric  1999        0.042574  5.701722e-02 -0.092194  1999   \n",
      "1      Timing1_Y  numeric  1999        0.114716  2.707722e-07  0.249778  1999   \n",
      "2      State_Y-1  numeric  1999        0.010674  6.333819e-01  0.022996  1999   \n",
      "3      State_Y-1  numeric  1999        0.010674  6.333819e-01  0.022996  1999   \n",
      "4   Position_Y-1  numeric  1999        0.001542  9.450813e-01 -0.003349  1999   \n",
      "\n",
      "   assoc_Y_metric       p_Y      MI_T      MI_Y  \\\n",
      "0        0.964235  0.000000  0.000000  1.338826   \n",
      "1        0.013398  0.549383  0.007308  0.004418   \n",
      "2        0.035922  0.108362  0.006689  0.000000   \n",
      "3        0.035922  0.108362  0.006689  0.000000   \n",
      "4        0.007910  0.723767  0.000000  0.000000   \n",
      "\n",
      "                                      classification  \n",
      "0                  prognostic_only (BOM p/ precisão)  \n",
      "1  instrument_like (EVITAR na modelagem causal; p...  \n",
      "2                         weak/irrelevant (OPCIONAL)  \n",
      "3                         weak/irrelevant (OPCIONAL)  \n",
      "4                         weak/irrelevant (OPCIONAL)  \n"
     ]
    }
   ],
   "source": [
    "# 1) Escolha as colunas candidatas\n",
    "candidate_cols = ['Timing1_Y','Position_Y-1','State_Y-1','State_Y-1','Outcoming_Y-1']\n",
    "\n",
    "# 2) Rode o scanner\n",
    "assoc_table = scan_associations(\n",
    "    df=df,\n",
    "    treatment_col='Tratamento',\n",
    "    outcome_col='Outcoming_Y',\n",
    "    candidate_features=candidate_cols,\n",
    "    alpha=0.05,\n",
    "    thr_corr_T=0.10,\n",
    "    thr_corr_Y=0.10,\n",
    "    thr_eta2=0.02\n",
    ")\n",
    "\n",
    "# 3) Veja a tabela/resultados\n",
    "print(assoc_table.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf4935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523aeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def nearest_neighbor_match(\n",
    "    df,\n",
    "    k=2,\n",
    "    with_replacement=False,                 # << escolha: True = com reposição | False = sem reposição\n",
    "    caliper_factor=0.25,                    # largura = fator * DP(score) nos CONTROLES\n",
    "    strata_cols=('Position_Y-1', 'State_Y-1'),  # colunas para matching EXATO por estrato\n",
    "    treatment_col='Tratamento',\n",
    "    ps_col='ps',                            # caso queira usar PS direto\n",
    "    logit_ps_col='logit_ps',                # coluna com logit(PS); será criada se não existir e houver 'ps'\n",
    "    use_logit_score=True,                   # True = usar logit(PS) no cálculo de distância/caliper\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Matching k:1 com/sem reposição + caliper + exato por estrato.\n",
    "    Retorna um DataFrame com colunas: ['t_idx', 'c_idx', 'w'].\n",
    "    'w' é o peso DENTRO do tratado (soma=1 por tratado emparelhado).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Score a ser usado (logit(PS) por padrão)\n",
    "    if use_logit_score:\n",
    "        if logit_ps_col not in df.columns:\n",
    "            if ps_col not in df.columns:\n",
    "                raise ValueError(\"Informe 'logit_ps' ou 'ps' na base.\")\n",
    "            p = np.clip(df[ps_col].to_numpy(), 1e-6, 1-1e-6)\n",
    "            df[logit_ps_col] = np.log(p/(1-p))\n",
    "        score_col = logit_ps_col\n",
    "    else:\n",
    "        if ps_col not in df.columns:\n",
    "            raise ValueError(\"use_logit_score=False exige coluna 'ps'.\")\n",
    "        score_col = ps_col\n",
    "\n",
    "    # 2) Estratos para matching exato (ou 'all' se não quiser estratificar)\n",
    "    if strata_cols and len(strata_cols) > 0:\n",
    "        df['strata'] = df.loc[:, list(strata_cols)].astype(str).agg('||'.join, axis=1)\n",
    "    else:\n",
    "        df['strata'] = 'all'\n",
    "\n",
    "    treated_idx = df.index[df[treatment_col] == 1]\n",
    "    control_idx = df.index[df[treatment_col] == 0]\n",
    "\n",
    "    # 3) Caliper (DP do score calculada nos controles)\n",
    "    caliper = caliper_factor * np.std(df.loc[control_idx, score_col])\n",
    "    if np.isnan(caliper) or caliper <= 0:\n",
    "        raise ValueError(\"Caliper inválido (verifique score nos controles).\")\n",
    "\n",
    "    # 4) Índices de controles por estrato (para matching exato)\n",
    "    strata_to_ctrl = {}\n",
    "    for i in control_idx:\n",
    "        strata_to_ctrl.setdefault(df.at[i, 'strata'], []).append(i)\n",
    "\n",
    "    pairs = []            # (t_idx, c_idx, w)\n",
    "    dists = []            # diagnósticos de distância\n",
    "    used_controls = set() # impede reuso quando with_replacement=False\n",
    "\n",
    "    # 5) Loop nos tratados\n",
    "    for ti in treated_idx:\n",
    "        key = df.at[ti, 'strata']\n",
    "        pool_all = strata_to_ctrl.get(key, [])\n",
    "        if not pool_all:\n",
    "            continue\n",
    "\n",
    "        # pool elegível (considera reuso ou não)\n",
    "        if with_replacement:\n",
    "            pool = pool_all\n",
    "        else:\n",
    "            pool = [j for j in pool_all if j not in used_controls]\n",
    "            if not pool:\n",
    "                continue\n",
    "\n",
    "        cand = np.array(pool)\n",
    "        d = np.abs(df.loc[cand, score_col].values - df.at[ti, score_col])\n",
    "\n",
    "        # aplica caliper\n",
    "        in_caliper = d <= caliper\n",
    "        if not np.any(in_caliper):\n",
    "            continue\n",
    "\n",
    "        cand = cand[in_caliper]\n",
    "        d = d[in_caliper]\n",
    "\n",
    "        # escolhe até k mais próximos\n",
    "        order = np.argsort(d)\n",
    "        chosen = cand[order][:k]\n",
    "\n",
    "        if len(chosen) == 0:\n",
    "            continue\n",
    "\n",
    "        w = 1.0 / len(chosen)  # pesos iguais dentro do tratado\n",
    "        for cj in chosen:\n",
    "            pairs.append((ti, cj, w))\n",
    "            dists.append(abs(df.at[cj, score_col] - df.at[ti, score_col]))\n",
    "            if not with_replacement:\n",
    "                used_controls.add(cj)\n",
    "\n",
    "    pairs = pd.DataFrame(pairs, columns=['t_idx', 'c_idx', 'w'])\n",
    "\n",
    "    # 6) Logs úteis\n",
    "    if verbose:\n",
    "        n_trat_total    = int((df[treatment_col] == 1).sum())\n",
    "        n_trat_pareados = pairs['t_idx'].nunique() if not pairs.empty else 0\n",
    "        n_ctrl_unicos   = pairs['c_idx'].nunique() if not pairs.empty else 0\n",
    "        tag = \"com REUSO\" if with_replacement else \"sem reuso\"\n",
    "\n",
    "        print(f\"\\nPareamento ({tag}): {n_trat_pareados} tratados pareados de {n_trat_total} \"\n",
    "              f\"({n_trat_pareados/max(1,n_trat_total):.1%}). Links: {len(pairs)}\")\n",
    "        print(f\"Controles únicos usados: {n_ctrl_unicos}\" + (\"\" if with_replacement else \" (reuso proibido)\"))\n",
    "\n",
    "        if len(dists) > 0:\n",
    "            dists = np.array(dists)\n",
    "            print(\"Distância no score: mediana={:.4f} | P90={:.4f} | máx={:.4f}\"\n",
    "                  .format(np.median(dists), np.quantile(dists, 0.90), np.max(dists)))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "# -------------------------\n",
    "# EXEMPLOS DE USO:\n",
    "# -------------------------\n",
    "# 1) Sem reposição (equivalente ao seu código original, mas parametrizado)\n",
    "# pairs_no_reuse = nearest_neighbor_match(df, k=2, with_replacement=False,\n",
    "#                                         caliper_factor=0.25,\n",
    "#                                         strata_cols=('Position_Y-1','State_Y-1'),\n",
    "#                                         treatment_col='Tratamento',\n",
    "#                                         logit_ps_col='logit_ps', use_logit_score=True)\n",
    "\n",
    "# 2) Com reposição (permite que um mesmo controle sirva a múltiplos tratados)\n",
    "# pairs_with_reuse = nearest_neighbor_match(df, k=2, with_replacement=True,\n",
    "#                                           caliper_factor=0.25,\n",
    "#                                           strata_cols=('Position_Y-1','State_Y-1'),\n",
    "#                                           treatment_col='Tratamento',\n",
    "#                                           logit_ps_col='logit_ps', use_logit_score=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
